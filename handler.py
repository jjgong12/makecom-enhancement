import runpod
import os
import json
import time
import base64
import numpy as np
from io import BytesIO
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont
import cv2
import logging
import re
import replicate
import requests
import string
import urllib.request
import urllib.error

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

################################
# ENHANCEMENT HANDLER - 1200x1560
# VERSION: V32-Ultra-Detailed-BG-Removal
################################

VERSION = "V32-Ultra-Detailed-BG-Removal"

# ===== GLOBAL INITIALIZATION =====
REPLICATE_API_TOKEN = os.environ.get('REPLICATE_API_TOKEN')
REPLICATE_CLIENT = None

if REPLICATE_API_TOKEN:
    try:
        REPLICATE_CLIENT = replicate.Client(api_token=REPLICATE_API_TOKEN)
        logger.info("âœ… Replicate client initialized")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Replicate: {e}")

# Global rembg session with U2Net
REMBG_SESSION = None

# Korean font cache - COMPLETELY FIXED
KOREAN_FONT_PATH = None
KOREAN_FONT_VERIFIED = False
FONT_CACHE = {}

def init_rembg_session():
    """Initialize rembg session with U2Net for faster processing"""
    global REMBG_SESSION
    if REMBG_SESSION is None:
        try:
            from rembg import new_session
            REMBG_SESSION = new_session('u2net')
            logger.info("âœ… U2Net session initialized for faster processing")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize rembg: {e}")
            REMBG_SESSION = None
    return REMBG_SESSION

# Initialize on module load
init_rembg_session()

def download_korean_font():
    """Download and verify Korean font - ENCODING FIXED"""
    global KOREAN_FONT_PATH, KOREAN_FONT_VERIFIED
    
    if KOREAN_FONT_PATH and KOREAN_FONT_VERIFIED:
        logger.info(f"âœ… Using cached Korean font: {KOREAN_FONT_PATH}")
        return KOREAN_FONT_PATH
    
    try:
        font_path = '/tmp/NanumGothic.ttf'
        
        # Always remove existing font file to ensure fresh download
        if os.path.exists(font_path):
            os.remove(font_path)
            logger.info("ğŸ”„ Removed existing font file for fresh download")
        
        # FIXED: Working font sources with proper URLs
        font_sources = [
            {
                'url': 'https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Regular.ttf',
                'name': 'Google Fonts GitHub (Primary)'
            },
            {
                'url': 'https://fonts.gstatic.com/s/nanumgothic/v17/PN_3Rfi-oW3hYwmKDpxS7F_D-d7qPgJc.ttf',
                'name': 'Google Fonts Direct'
            },
            {
                'url': 'https://cdn.jsdelivr.net/gh/google/fonts@main/ofl/nanumgothic/NanumGothic-Regular.ttf',
                'name': 'JSDelivr CDN'
            },
            {
                'url': 'https://raw.githubusercontent.com/google/fonts/main/ofl/nanumgothic/NanumGothic-Regular.ttf',
                'name': 'GitHub Raw'
            }
        ]
        
        for source in font_sources:
            try:
                logger.info(f"ğŸ”½ Downloading Korean font from: {source['name']}")
                
                # Use urllib for better control
                req = urllib.request.Request(
                    source['url'],
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Accept': 'application/font-ttf,font/*,*/*'
                    }
                )
                
                with urllib.request.urlopen(req, timeout=30) as response:
                    font_data = response.read()
                
                # Validate font data
                if len(font_data) < 50000:  # TTF files should be at least 50KB
                    logger.warning(f"âŒ Font file too small: {len(font_data)} bytes")
                    continue
                
                # Save font file
                with open(font_path, 'wb') as f:
                    f.write(font_data)
                
                # CRITICAL: Verify Korean font works
                if verify_korean_font(font_path):
                    KOREAN_FONT_PATH = font_path
                    KOREAN_FONT_VERIFIED = True
                    logger.info(f"âœ… Korean font successfully downloaded and verified: {source['name']}")
                    return font_path
                else:
                    logger.warning(f"âŒ Font verification failed for: {source['name']}")
                    if os.path.exists(font_path):
                        os.remove(font_path)
                
            except Exception as e:
                logger.error(f"âŒ Failed to download from {source['name']}: {e}")
                continue
        
        # If all downloads fail, try to use system fonts
        logger.warning("ğŸ”„ All downloads failed, trying system fonts...")
        system_fonts = [
            '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
            '/usr/share/fonts/TTF/NanumGothic.ttf',
            '/System/Library/Fonts/AppleSDGothicNeo.ttc',
            '/Windows/Fonts/malgun.ttf',
            '/System/Library/Fonts/AppleGothic.ttf'
        ]
        
        for sys_font in system_fonts:
            if os.path.exists(sys_font):
                if verify_korean_font(sys_font):
                    KOREAN_FONT_PATH = sys_font
                    KOREAN_FONT_VERIFIED = True
                    logger.info(f"âœ… Using system Korean font: {sys_font}")
                    return sys_font
        
        logger.error("âŒ No valid Korean font found")
        return None
        
    except Exception as e:
        logger.error(f"âŒ Font download error: {e}")
        return None

def verify_korean_font(font_path):
    """Verify that the font can render Korean text properly - FIXED"""
    try:
        # Test with multiple Korean texts
        test_texts = [
            "í•œê¸€",
            "í…ŒìŠ¤íŠ¸", 
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ì´ ì œí’ˆì€",
            "ê°ë„ì— ë”°ë¼",
            "MD TALK",
            "DESIGN POINT"
        ]
        
        # Test multiple font sizes
        for size in [20, 24, 28, 48]:
            # FIXED: Remove encoding parameter completely
            font = ImageFont.truetype(font_path, size)
            
            # Create test image
            test_img = Image.new('RGB', (500, 200), 'white')
            draw = ImageDraw.Draw(test_img)
            
            y_pos = 10
            for text in test_texts:
                try:
                    # Test text rendering with proper UTF-8 string
                    bbox = draw.textbbox((10, y_pos), text, font=font)
                    draw.text((10, y_pos), text, font=font, fill='black')
                    
                    # Check if bbox is valid
                    if bbox[2] - bbox[0] <= 0 or bbox[3] - bbox[1] <= 0:
                        logger.warning(f"âŒ Invalid bbox for text: {text}")
                        return False
                    
                    y_pos += 25
                    
                except Exception as e:
                    logger.warning(f"âŒ Failed to render text '{text}': {e}")
                    return False
        
        logger.info("âœ… Korean font verification passed")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Font verification error: {e}")
        return False

def get_font(size, force_korean=True):
    """Get font with Korean support - ENCODING FIXED"""
    global KOREAN_FONT_PATH, FONT_CACHE
    
    cache_key = f"{size}_{force_korean}"
    
    # Return cached font if available
    if cache_key in FONT_CACHE:
        return FONT_CACHE[cache_key]
    
    font = None
    
    if force_korean:
        # Ensure Korean font is available
        if not KOREAN_FONT_PATH or not KOREAN_FONT_VERIFIED:
            korean_path = download_korean_font()
            if not korean_path:
                logger.error("âŒ No Korean font available")
                force_korean = False
        
        if KOREAN_FONT_PATH and os.path.exists(KOREAN_FONT_PATH):
            try:
                # FIXED: Remove encoding parameter completely
                font = ImageFont.truetype(KOREAN_FONT_PATH, size)
                logger.info(f"âœ… Korean font loaded: size {size}")
            except Exception as e:
                logger.error(f"âŒ Failed to load Korean font: {e}")
                font = None
    
    # Fallback to default font
    if font is None:
        try:
            font = ImageFont.load_default()
            logger.warning(f"âš ï¸ Using default font for size {size}")
        except Exception as e:
            logger.error(f"âŒ Failed to load default font: {e}")
            font = None
    
    # Cache the font
    if font:
        FONT_CACHE[cache_key] = font
    
    return font

def safe_draw_text(draw, position, text, font, fill):
    """Safely draw Korean text - ENCODING FIXED"""
    try:
        if not text or not font:
            logger.warning("âš ï¸ No text or font provided")
            return
        
        # FIXED: Simplified encoding handling - just ensure UTF-8 string
        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='replace')
        
        text = str(text).strip()
        
        if not text:
            logger.warning("âš ï¸ Empty text after processing")
            return
        
        # Draw the text directly - PIL handles Korean properly now
        draw.text(position, text, font=font, fill=fill)
        logger.info(f"âœ… Successfully drew text: {text[:20]}...")
        
    except Exception as e:
        logger.error(f"âŒ Text drawing error: {e}, text: {repr(text)}")
        # Simple fallback
        try:
            if font:
                draw.text(position, "[í•œê¸€ ì˜¤ë¥˜]", font=font, fill=fill)
        except:
            pass

def get_text_size(draw, text, font):
    """Get text size - FIXED for Korean"""
    try:
        if not text or not font:
            return (0, 0)
        
        # Handle encoding
        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='replace')
        text = str(text).strip()
        
        if not text:
            return (0, 0)
        
        # Use textbbox for accurate measurement
        bbox = draw.textbbox((0, 0), text, font=font)
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        
        return (max(0, width), max(0, height))
        
    except Exception as e:
        logger.error(f"âŒ Text size calculation error: {e}")
        return (100, 20)  # Fallback size

def wrap_text(text, font, max_width, draw):
    """Wrap text to fit within max_width"""
    words = text.split()
    lines = []
    current_line = []
    
    for word in words:
        test_line = ' '.join(current_line + [word])
        bbox = draw.textbbox((0, 0), test_line, font=font)
        width = bbox[2] - bbox[0]
        
        if width <= max_width:
            current_line.append(word)
        else:
            if current_line:
                lines.append(' '.join(current_line))
                current_line = [word]
            else:
                # Word is too long, add it anyway
                lines.append(word)
                current_line = []
    
    if current_line:
        lines.append(' '.join(current_line))
    
    return lines

def create_md_talk_section(text_content=None, width=1200):
    """Create MD TALK section - FIXED SIZE 1200x600"""
    logger.info("ğŸ”¤ Creating MD TALK section with FIXED size 1200x600")
    
    # Fixed dimensions
    fixed_width = 1200
    fixed_height = 600
    
    # Margins
    left_margin = 100
    right_margin = 100
    top_margin = 80
    content_width = fixed_width - left_margin - right_margin
    
    # Ensure Korean font is ready
    download_korean_font()
    
    # Get fonts with proper sizes
    title_font = get_font(48, force_korean=True)
    body_font = get_font(28, force_korean=True)
    
    if not title_font or not body_font:
        logger.error("âŒ Failed to load fonts for MD TALK")
        # Create error image
        error_img = Image.new('RGB', (fixed_width, fixed_height), '#FFFFFF')
        error_draw = ImageDraw.Draw(error_img)
        try:
            error_draw.text((50, 50), "Font Error - MD TALK", fill='red')
        except:
            pass
        return error_img
    
    # Create final image with fixed size
    section_img = Image.new('RGB', (fixed_width, fixed_height), '#FFFFFF')
    draw = ImageDraw.Draw(section_img)
    
    # Title
    title = "MD TALK"
    title_width, title_height = get_text_size(draw, title, title_font)
    
    # Draw title centered
    title_x = (fixed_width - title_width) // 2
    safe_draw_text(draw, (title_x, top_margin), title, title_font, (40, 40, 40))
    
    # Prepare Korean text content
    if text_content and text_content.strip():
        text = text_content.replace('MD TALK', '').replace('MD Talk', '').strip()
    else:
        text = """ì´ ì œí’ˆì€ ì¼ìƒì—ì„œë„ ë¶€ë‹´ì—†ì´ ì°©ìš©í•  ìˆ˜ ìˆëŠ” í¸ì•ˆí•œ ë””ìì¸ìœ¼ë¡œ ë§¤ì¼ì˜ ìŠ¤íƒ€ì¼ë§ì— í¬ì¸íŠ¸ë¥¼ ë”í•´ì¤ë‹ˆë‹¤. íŠ¹ë³„í•œ ë‚ ì€ ë¬¼ë¡  í‰ë²”í•œ ì¼ìƒê¹Œì§€ ëª¨ë“  ìˆœê°„ì„ ë¹›ë‚˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë‹¹ì‹ ë§Œì˜ íŠ¹ë³„í•œ ì£¼ì–¼ë¦¬ì…ë‹ˆë‹¤."""
    
    # Ensure UTF-8 string
    if isinstance(text, bytes):
        text = text.decode('utf-8', errors='replace')
    text = str(text).strip()
    
    # Wrap text to fit within margins
    wrapped_lines = wrap_text(text, body_font, content_width, draw)
    
    # Calculate vertical position for centering content
    line_height = 50
    content_height = len(wrapped_lines) * line_height
    title_bottom_margin = 80
    
    # Start position for body text
    y_pos = top_margin + title_height + title_bottom_margin
    
    # Center text vertically if space allows
    total_content_height = title_height + title_bottom_margin + content_height
    if total_content_height < fixed_height - top_margin - 80:
        y_pos = (fixed_height - content_height) // 2 + 20
    
    # Draw body text with center alignment
    for line in wrapped_lines:
        if line:
            line_width, _ = get_text_size(draw, line, body_font)
            line_x = (fixed_width - line_width) // 2
            safe_draw_text(draw, (line_x, y_pos), line, body_font, (80, 80, 80))
        y_pos += line_height
    
    logger.info(f"âœ… MD TALK section created: {fixed_width}x{fixed_height}")
    return section_img

def create_design_point_section(text_content=None, width=1200):
    """Create DESIGN POINT section - FIXED SIZE 1200x600"""
    logger.info("ğŸ”¤ Creating DESIGN POINT section with FIXED size 1200x600")
    
    # Fixed dimensions
    fixed_width = 1200
    fixed_height = 600
    
    # Margins
    left_margin = 100
    right_margin = 100
    top_margin = 80
    content_width = fixed_width - left_margin - right_margin
    
    # Ensure Korean font is ready
    download_korean_font()
    
    # Get fonts
    title_font = get_font(48, force_korean=True)
    body_font = get_font(24, force_korean=True)
    
    if not title_font or not body_font:
        logger.error("âŒ Failed to load fonts for DESIGN POINT")
        # Create error image
        error_img = Image.new('RGB', (fixed_width, fixed_height), '#FFFFFF')
        error_draw = ImageDraw.Draw(error_img)
        try:
            error_draw.text((50, 50), "Font Error - DESIGN POINT", fill='red')
        except:
            pass
        return error_img
    
    # Create final image with fixed size
    section_img = Image.new('RGB', (fixed_width, fixed_height), '#FFFFFF')
    draw = ImageDraw.Draw(section_img)
    
    # Title
    title = "DESIGN POINT"
    title_width, title_height = get_text_size(draw, title, title_font)
    
    # Draw title centered
    title_x = (fixed_width - title_width) // 2
    safe_draw_text(draw, (title_x, top_margin), title, title_font, (40, 40, 40))
    
    # Prepare text content
    if text_content and text_content.strip():
        text = text_content.replace('DESIGN POINT', '').replace('Design Point', '').strip()
    else:
        text = """ë‚¨ì„± ë‹¨í’ˆì€ ë¬´ê´‘ í…ìŠ¤ì²˜ì™€ ìœ ê´‘ ë¼ì¸ì˜ ì¡°í™”ê°€ ê²¬ê³ í•œ ê°ì„±ì„ ì „í•˜ê³  ì—¬ì ë‹¨í’ˆì€ íŒŒë²  ì„¸íŒ…ê³¼ ì„¬ì„¸í•œ ë°€ê·¸ë ˆì¸ì˜ ë””í…Œì¼ í™”ë ¤í•˜ë©´ì„œë„ ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ë°˜ì§ì„ì„ í‘œí˜„í•©ë‹ˆë‹¤"""
    
    # Ensure UTF-8 string
    if isinstance(text, bytes):
        text = text.decode('utf-8', errors='replace')
    text = str(text).strip()
    
    # Wrap text to fit within margins
    wrapped_lines = wrap_text(text, body_font, content_width, draw)
    
    # Calculate vertical position for centering content
    line_height = 45
    content_height = len(wrapped_lines) * line_height
    title_bottom_margin = 100
    
    # Start position for body text
    y_pos = top_margin + title_height + title_bottom_margin
    
    # Center text vertically if space allows
    total_content_height = title_height + title_bottom_margin + content_height
    if total_content_height < fixed_height - top_margin - 100:
        y_pos = (fixed_height - content_height) // 2 + 40
    
    # Draw body text with center alignment
    for line in wrapped_lines:
        if line:
            line_width, _ = get_text_size(draw, line, body_font)
            line_x = (fixed_width - line_width) // 2
            safe_draw_text(draw, (line_x, y_pos), line, body_font, (80, 80, 80))
        y_pos += line_height
    
    # Draw bottom line
    line_y = fixed_height - 80
    draw.rectangle([100, line_y, fixed_width - 100, line_y + 2], fill=(220, 220, 220))
    
    logger.info(f"âœ… DESIGN POINT section created: {fixed_width}x{fixed_height}")
    return section_img

def u2net_ultra_precise_removal_v2(image: Image.Image) -> Image.Image:
    """ULTRA PRECISE U2Net background removal with ENHANCED multi-stage processing"""
    try:
        from rembg import remove
        
        global REMBG_SESSION
        if REMBG_SESSION is None:
            REMBG_SESSION = init_rembg_session()
            if REMBG_SESSION is None:
                return image
        
        logger.info("ğŸ”· U2Net ULTRA PRECISE Background Removal V32 - ENHANCED")
        logger.info("ğŸ¯ Stage 1/6: Initial U2Net Processing")
        
        # CRITICAL: Ensure RGBA mode before processing
        if image.mode != 'RGBA':
            if image.mode == 'RGB':
                image = image.convert('RGBA')
            else:
                image = image.convert('RGBA')
        
        # ========== STAGE 1: PREPROCESSING ==========
        logger.info("ğŸ“ Pre-processing for optimal U2Net performance")
        
        # 1.1 Enhance contrast for better edge detection
        contrast = ImageEnhance.Contrast(image)
        image_enhanced = contrast.enhance(1.12)
        
        # 1.2 Slight sharpening to emphasize edges
        sharpness = ImageEnhance.Sharpness(image_enhanced)
        image_enhanced = sharpness.enhance(1.08)
        
        # 1.3 Color enhancement for better separation
        color = ImageEnhance.Color(image_enhanced)
        image_enhanced = color.enhance(1.05)
        
        # Save image to buffer
        buffered = BytesIO()
        image_enhanced.save(buffered, format="PNG", compress_level=0)
        buffered.seek(0)
        img_data = buffered.getvalue()
        
        # ========== STAGE 2: U2NET PROCESSING ==========
        logger.info("ğŸ“ Applying U2Net with ULTRA PRECISE settings")
        
        # Apply U2Net removal with ULTRA PRECISE settings
        output = remove(
            img_data,
            session=REMBG_SESSION,
            alpha_matting=True,
            alpha_matting_foreground_threshold=275,  # Higher for ultra precision
            alpha_matting_background_threshold=5,
            alpha_matting_erode_size=0,  # No erosion for max detail
            only_mask=False,
            post_process_mask=True  # Enable post-processing
        )
        
        result_image = Image.open(BytesIO(output))
        
        # CRITICAL: Ensure RGBA mode
        if result_image.mode != 'RGBA':
            result_image = result_image.convert('RGBA')
        
        # ========== STAGE 3: EDGE REFINEMENT ==========
        logger.info("ğŸ¯ Stage 2/6: Advanced Edge Refinement")
        
        r, g, b, a = result_image.split()
        alpha_array = np.array(a, dtype=np.uint8)
        
        # Convert to float for processing
        alpha_float = alpha_array.astype(np.float32) / 255.0
        
        # 3.1 Multi-scale edge detection
        rgb_array = np.array(result_image.convert('RGB'), dtype=np.uint8)
        gray = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2GRAY)
        
        # Fine edges using Canny
        edges_fine = cv2.Canny(gray, 50, 150)
        
        # Medium edges with slight blur
        gray_blur = cv2.GaussianBlur(gray, (3, 3), 1.0)
        edges_medium = cv2.Canny(gray_blur, 30, 100)
        
        # Coarse edges using Sobel
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        edge_magnitude = np.sqrt(sobelx**2 + sobely**2)
        edge_magnitude = (edge_magnitude / edge_magnitude.max() * 255).astype(np.uint8)
        edges_coarse = edge_magnitude > 35
        
        # Combine all edge scales
        combined_edges = edges_fine | edges_medium | edges_coarse.astype(np.uint8) * 255
        
        # Create edge mask with dilation
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        edge_mask = cv2.dilate(combined_edges, kernel, iterations=2)
        
        # 3.2 Apply guided filter for ultra-smooth edges
        logger.info("ğŸ“ Applying guided filter for edge-aware smoothing")
        try:
            # Normalize gray for guided filter
            gray_float = gray.astype(np.float32) / 255.0
            
            # Multiple passes of guided filter with different parameters
            alpha_guided1 = cv2.ximgproc.guidedFilter(
                guide=gray_float,
                src=alpha_float,
                radius=1,
                eps=0.00005  # Ultra small epsilon for maximum edge preservation
            )
            
            alpha_guided2 = cv2.ximgproc.guidedFilter(
                guide=gray_float,
                src=alpha_guided1,
                radius=3,
                eps=0.0005
            )
            
            alpha_guided3 = cv2.ximgproc.guidedFilter(
                guide=gray_float,
                src=alpha_guided2,
                radius=5,
                eps=0.001
            )
            
            # Weighted blend of guided results
            alpha_float = alpha_guided1 * 0.5 + alpha_guided2 * 0.3 + alpha_guided3 * 0.2
            
        except AttributeError:
            logger.warning("âš ï¸ Guided filter not available, using bilateral filter")
            # Fallback to bilateral filter
            alpha_uint8 = (alpha_float * 255).astype(np.uint8)
            alpha_bilateral = cv2.bilateralFilter(alpha_uint8, 9, 75, 75)
            alpha_float = alpha_bilateral.astype(np.float32) / 255.0
        
        # 3.3 Ultra-precise threshold with smooth gradients
        logger.info("ğŸ“ Applying sigmoid curve for smooth transitions")
        k = 45  # Steepness of transition
        threshold = 0.48
        alpha_sigmoid = 1 / (1 + np.exp(-k * (alpha_float - threshold)))
        
        # ========== STAGE 4: REFLECTION & METALLIC HANDLING ==========
        logger.info("ğŸ¯ Stage 3/6: Reflection and Metallic Surface Handling")
        
        # 4.1 Detect metallic and reflective areas
        hsv = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2HSV)
        h_channel, s_channel, v_channel = cv2.split(hsv)
        
        # Metallic surfaces: high brightness, low saturation
        metallic_mask = (v_channel > 200) & (s_channel < 40)
        
        # Specular highlights
        _, highlights = cv2.threshold(gray, 245, 255, cv2.THRESH_BINARY)
        
        # Smooth bright areas (potential metal)
        gradx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grady = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_mag = np.sqrt(gradx**2 + grady**2)
        smooth_bright = (v_channel > 180) & (gradient_mag < 25)
        
        # Combine reflective indicators
        reflective_areas = metallic_mask | (highlights > 0) | smooth_bright
        
        # Clean up reflective mask
        kernel_clean = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        reflective_areas = cv2.morphologyEx(reflective_areas.astype(np.uint8), 
                                            cv2.MORPH_CLOSE, kernel_clean)
        
        # 4.2 Refine alpha in reflective areas
        if np.any(reflective_areas):
            logger.info("ğŸ“ Refining alpha in reflective/metallic areas")
            
            # Get median color in reflective regions
            reflective_pixels = rgb_array[reflective_areas > 0]
            if len(reflective_pixels) > 0:
                median_color = np.median(reflective_pixels, axis=0)
                
                # Create distance map from median color
                color_distance = np.sqrt(np.sum((rgb_array - median_color) ** 2, axis=2))
                
                # Normalize distance
                color_distance_norm = color_distance / (np.max(color_distance) + 1e-8)
                
                # In reflective areas, adjust alpha based on color consistency
                reflective_mask_float = reflective_areas.astype(np.float32) / 255.0
                alpha_sigmoid = alpha_sigmoid * (1 - reflective_mask_float * 0.3) + \
                               (1 - color_distance_norm) * reflective_mask_float * 0.3
        
        # ========== STAGE 5: HAIR AND FINE DETAIL RECOVERY ==========
        logger.info("ğŸ¯ Stage 4/6: Hair and Fine Detail Recovery")
        
        # 5.1 Detect fine details using high-pass filter
        blur_for_details = cv2.GaussianBlur(gray, (7, 7), 2.0)
        high_pass = cv2.subtract(gray, blur_for_details)
        
        # Threshold to get detail mask
        _, detail_mask = cv2.threshold(np.abs(high_pass), 8, 255, cv2.THRESH_BINARY)
        
        # 5.2 Hair-specific detection
        # Hair often has directional patterns
        gabor_kernels = []
        for theta in np.arange(0, np.pi, np.pi / 8):
            kernel = cv2.getGaborKernel((21, 21), 4.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)
            gabor_kernels.append(kernel)
        
        hair_response = np.zeros_like(gray, dtype=np.float32)
        for kernel in gabor_kernels:
            filtered = cv2.filter2D(gray, cv2.CV_32F, kernel)
            hair_response = np.maximum(hair_response, np.abs(filtered))
        
        hair_mask = hair_response > np.percentile(hair_response, 85)
        
        # 5.3 Recover lost details
        fine_details = (detail_mask > 0) | hair_mask
        
        # Check connectivity to main object
        num_labels, labels = cv2.connectedComponents(fine_details.astype(np.uint8))
        
        for label in range(1, num_labels):
            component = (labels == label)
            
            # Dilate to check connectivity
            dilated = cv2.dilate(component.astype(np.uint8), np.ones((7, 7)), iterations=2)
            
            # If connected to solid foreground, preserve it
            if np.any((dilated > 0) & (alpha_sigmoid > 0.7)):
                alpha_sigmoid[component] = np.maximum(alpha_sigmoid[component], 0.6)
        
        # ========== STAGE 6: ARTIFACT REMOVAL ==========
        logger.info("ğŸ¯ Stage 5/6: Removing Small Artifacts")
        
        # 6.1 Remove small disconnected components
        alpha_binary = (alpha_sigmoid > 0.5).astype(np.uint8)
        num_labels, labels = cv2.connectedComponents(alpha_binary)
        
        if num_labels > 2:
            sizes = [np.sum(labels == i) for i in range(1, num_labels)]
            if sizes:
                # Adaptive size threshold based on image size
                min_size = int(alpha_array.size * 0.0003)  # 0.03% of image
                valid_labels = [i+1 for i, size in enumerate(sizes) if size > min_size]
                
                # Create valid mask
                valid_mask = np.zeros_like(alpha_binary, dtype=bool)
                for label in valid_labels:
                    valid_mask |= (labels == label)
                
                # Apply to non-edge areas only
                non_edge_mask = ~edge_mask.astype(bool)
                alpha_sigmoid[~valid_mask & non_edge_mask] = 0
        
        # 6.2 Fill small holes
        # Invert alpha to find holes
        alpha_inverted = 1 - alpha_sigmoid
        alpha_inv_binary = (alpha_inverted > 0.5).astype(np.uint8)
        num_labels_inv, labels_inv = cv2.connectedComponents(alpha_inv_binary)
        
        if num_labels_inv > 2:
            sizes_inv = [np.sum(labels_inv == i) for i in range(1, num_labels_inv)]
            if sizes_inv:
                # Fill holes smaller than threshold
                max_hole_size = int(alpha_array.size * 0.0005)  # 0.05% of image
                
                for i, size in enumerate(sizes_inv):
                    if size < max_hole_size:
                        hole_mask = (labels_inv == i+1)
                        # Get average alpha around hole
                        dilated_hole = cv2.dilate(hole_mask.astype(np.uint8), 
                                                  np.ones((5, 5)), iterations=1)
                        border = dilated_hole & ~hole_mask
                        
                        if np.any(border):
                            avg_alpha = np.mean(alpha_sigmoid[border])
                            alpha_sigmoid[hole_mask] = avg_alpha
        
        # ========== STAGE 7: FINAL POLISH ==========
        logger.info("ğŸ¯ Stage 6/6: Final Polish and Edge Enhancement")
        
        # 7.1 Edge enhancement
        edge_enhancement = 1.15
        edge_zone = edge_mask > 0
        alpha_sigmoid[edge_zone] = np.clip(alpha_sigmoid[edge_zone] * edge_enhancement, 0, 1)
        
        # 7.2 Final feathering for natural edges
        # Convert back to uint8 temporarily
        alpha_array = np.clip(alpha_sigmoid * 255, 0, 255).astype(np.uint8)
        
        # Create feather zones
        kernel_feather = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        alpha_eroded = cv2.erode(alpha_array, kernel_feather, iterations=1)
        alpha_dilated = cv2.dilate(alpha_array, kernel_feather, iterations=1)
        
        feather_mask = (alpha_dilated > 0) & (alpha_eroded < 255)
        
        if np.any(feather_mask):
            # Graduated feathering
            alpha_float_temp = alpha_array.astype(np.float32)
            distance_to_edge = cv2.distanceTransform(alpha_eroded, cv2.DIST_L2, 3)
            distance_norm = np.clip(distance_to_edge / 3.0, 0, 1)
            
            alpha_float_temp[feather_mask] = (
                alpha_array[feather_mask] * (0.3 + 0.7 * distance_norm[feather_mask])
            )
            
            alpha_array = np.clip(alpha_float_temp, 0, 255).astype(np.uint8)
        
        # 7.3 Final smoothing pass
        # Apply final guided filter for ultra-smooth result
        try:
            alpha_final_float = alpha_array.astype(np.float32) / 255.0
            alpha_final_guided = cv2.ximgproc.guidedFilter(
                guide=gray_float,
                src=alpha_final_float,
                radius=2,
                eps=0.001
            )
            alpha_array = np.clip(alpha_final_guided * 255, 0, 255).astype(np.uint8)
        except:
            # Fallback smoothing
            alpha_array = cv2.medianBlur(alpha_array, 3)
        
        logger.info("âœ… ULTRA PRECISE background removal complete - RGBA preserved")
        
        # Final composition
        a_new = Image.fromarray(alpha_array)
        result = Image.merge('RGBA', (r, g, b, a_new))
        
        # Verify RGBA mode
        if result.mode != 'RGBA':
            logger.error("âŒ WARNING: Result is not RGBA!")
            result = result.convert('RGBA')
        
        return result
        
    except Exception as e:
        logger.error(f"U2Net removal failed: {e}")
        # Ensure RGBA mode even on failure
        if image.mode != 'RGBA':
            return image.convert('RGBA')
        return image

def ensure_ring_holes_transparent_ultra_v2(image: Image.Image) -> Image.Image:
    """ULTRA PRECISE ring hole detection with ENHANCED multi-criteria analysis"""
    # CRITICAL: Preserve RGBA mode
    if image.mode != 'RGBA':
        image = image.convert('RGBA')
    
    logger.info("ğŸ” ULTRA PRECISE Ring Hole Detection V32 - ENHANCED")
    
    r, g, b, a = image.split()
    alpha_array = np.array(a, dtype=np.uint8)
    rgb_array = np.array(image.convert('RGB'), dtype=np.uint8)
    
    h, w = alpha_array.shape
    
    # ========== STAGE 1: MULTI-CRITERIA HOLE DETECTION ==========
    logger.info("ğŸ“ Stage 1: Multi-criteria hole detection")
    
    # Convert to multiple color spaces
    hsv = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2HSV)
    h_channel, s_channel, v_channel = cv2.split(hsv)
    
    lab = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2LAB)
    l_channel = lab[:, :, 0]
    
    # Criteria 1: Very bright areas (expanded detection)
    very_bright = l_channel > 235
    
    # Criteria 2: Low saturation (nearly white/gray)
    low_saturation = s_channel < 25
    
    # Criteria 3: Already transparent areas
    alpha_holes = alpha_array < 40
    
    # Criteria 4: White color detection in RGB
    white_threshold = 240
    white_pixels = (rgb_array[:, :, 0] > white_threshold) & \
                   (rgb_array[:, :, 1] > white_threshold) & \
                   (rgb_array[:, :, 2] > white_threshold)
    
    # Criteria 5: Circular shape detection using Hough Transform
    logger.info("ğŸ“ Detecting circular patterns")
    gray = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2GRAY)
    
    # Apply slight blur for better circle detection
    gray_blur = cv2.GaussianBlur(gray, (5, 5), 1.0)
    
    # Detect circles
    circles = cv2.HoughCircles(
        gray_blur,
        cv2.HOUGH_GRADIENT,
        dp=1,
        minDist=20,
        param1=50,
        param2=25,  # Lower threshold for better detection
        minRadius=8,
        maxRadius=min(w, h) // 4
    )
    
    circle_mask = np.zeros_like(gray, dtype=bool)
    
    if circles is not None:
        circles = np.uint16(np.around(circles))
        for i in circles[0, :]:
            x, y, radius = i
            # Create circle mask
            temp_mask = np.zeros_like(gray)
            cv2.circle(temp_mask, (x, y), radius, 255, -1)
            
            # Check if circle content is bright
            circle_pixels = gray[temp_mask > 0]
            if len(circle_pixels) > 0:
                mean_brightness = np.mean(circle_pixels)
                brightness_std = np.std(circle_pixels)
                
                # Bright and uniform circles are likely holes
                if mean_brightness > 230 and brightness_std < 20:
                    circle_mask[temp_mask > 0] = True
    
    # Combine all criteria
    potential_holes = (very_bright & low_saturation) | alpha_holes | white_pixels | circle_mask
    
    # ========== STAGE 2: MORPHOLOGICAL CLEANING ==========
    logger.info("ğŸ“ Stage 2: Morphological cleaning")
    
    # Clean up noise with adaptive kernel size
    kernel_size = max(3, min(7, int(min(w, h) * 0.01)))
    kernel_clean = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    
    # Opening to remove small noise
    potential_holes = cv2.morphologyEx(potential_holes.astype(np.uint8), 
                                       cv2.MORPH_OPEN, kernel_clean)
    
    # Closing to fill small gaps
    potential_holes = cv2.morphologyEx(potential_holes, 
                                       cv2.MORPH_CLOSE, kernel_clean)
    
    # ========== STAGE 3: COMPONENT ANALYSIS ==========
    logger.info("ğŸ“ Stage 3: Analyzing connected components")
    
    # Find connected components
    num_labels, labels = cv2.connectedComponents(potential_holes)
    
    holes_mask = np.zeros_like(alpha_array, dtype=np.float32)
    
    # Analyze each component
    for label in range(1, num_labels):
        component = (labels == label)
        component_size = np.sum(component)
        
        # Size filtering - adaptive based on image size
        min_hole_size = h * w * 0.00008  # Smaller minimum for tiny holes
        max_hole_size = h * w * 0.15     # Maximum 15% of image
        
        if min_hole_size < component_size < max_hole_size:
            coords = np.where(component)
            if len(coords[0]) == 0:
                continue
                
            min_y, max_y = coords[0].min(), coords[0].max()
            min_x, max_x = coords[1].min(), coords[1].max()
            
            comp_width = max_x - min_x
            comp_height = max_y - min_y
            
            if comp_height == 0:
                continue
            
            # ===== ENHANCED VALIDATION CRITERIA =====
            
            # 1. Aspect ratio check (holes are usually somewhat circular)
            aspect_ratio = comp_width / comp_height
            aspect_valid = 0.25 < aspect_ratio < 4.0
            
            # 2. Position check - holes are usually near ring center
            center_y, center_x = (min_y + max_y) / 2, (min_x + max_x) / 2
            center_distance = np.sqrt((center_x - w/2)**2 + (center_y - h/2)**2)
            position_valid = center_distance < max(w, h) * 0.42
            
            # 3. Brightness analysis
            component_pixels = rgb_array[component]
            if len(component_pixels) > 0:
                brightness = np.mean(component_pixels)
                brightness_std = np.std(component_pixels)
                
                brightness_valid = brightness > 225
                consistency_valid = brightness_std < 30
            else:
                brightness_valid = False
                consistency_valid = False
            
            # 4. Circularity check
            component_uint8 = component.astype(np.uint8) * 255
            contours, _ = cv2.findContours(component_uint8, cv2.RETR_EXTERNAL, 
                                          cv2.CHAIN_APPROX_SIMPLE)
            
            circularity_valid = False
            if contours:
                contour = contours[0]
                area = cv2.contourArea(contour)
                perimeter = cv2.arcLength(contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    circularity_valid = circularity > 0.35
            
            # 5. Edge smoothness check
            edges = cv2.Canny(component_uint8, 50, 150)
            edge_pixels = np.sum(edges > 0)
            
            smoothness_valid = True
            if perimeter > 0:
                edge_ratio = edge_pixels / perimeter
                smoothness_valid = edge_ratio < 2.5
            
            # 6. Color uniformity check
            if len(component_pixels) > 10:
                # Check color variance within component
                color_variance = np.mean(np.var(component_pixels, axis=0))
                uniformity_valid = color_variance < 400  # Low variance = uniform color
            else:
                uniformity_valid = False
            
            # 7. Gradient check - holes usually have smooth gradients
            component_gray = gray[component]
            if len(component_gray) > 10:
                gradient = np.gradient(component_gray.astype(float))
                gradient_smoothness = np.mean(np.abs(gradient))
                gradient_valid = gradient_smoothness < 5
            else:
                gradient_valid = False
            
            # ===== CONFIDENCE SCORING =====
            confidence = 0.0
            
            # Weight different criteria
            if brightness_valid: confidence += 0.25
            if consistency_valid: confidence += 0.20
            if position_valid: confidence += 0.10
            if circularity_valid: confidence += 0.15
            if smoothness_valid: confidence += 0.10
            if uniformity_valid: confidence += 0.10
            if gradient_valid: confidence += 0.10
            
            # Bonus for meeting multiple criteria
            criteria_met = sum([brightness_valid, consistency_valid, position_valid, 
                               circularity_valid, smoothness_valid, uniformity_valid, 
                               gradient_valid])
            
            if criteria_met >= 5:
                confidence += 0.15
            
            # Apply if confidence is high enough and basic shape is valid
            if confidence > 0.40 and aspect_valid:
                # Graduated application based on confidence
                hole_strength = min(1.0, confidence * 1.5)
                holes_mask[component] = 255 * hole_strength
                
                logger.info(f"âœ… Hole detected - Confidence: {confidence:.2f}, "
                          f"Size: {component_size}, Position: ({center_x:.0f}, {center_y:.0f}), "
                          f"Brightness: {brightness:.0f}Â±{brightness_std:.0f}")
    
    # ========== STAGE 4: HOLE REFINEMENT ==========
    logger.info("ğŸ“ Stage 4: Hole refinement and smoothing")
    
    if np.any(holes_mask > 0):
        # 4.1 Smooth hole boundaries
        holes_mask_smooth = cv2.GaussianBlur(holes_mask, (7, 7), 1.5)
        
        # 4.2 Create transition zones
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))
        holes_dilated = cv2.dilate((holes_mask > 0).astype(np.uint8), 
                                   kernel_dilate, iterations=2)
        
        # Inner hole area (definitely transparent)
        holes_inner = cv2.erode((holes_mask > 200).astype(np.uint8), 
                               kernel_dilate, iterations=1)
        
        # Transition zone
        transition_zone = (holes_dilated > 0) & (holes_inner == 0)
        
        # ========== STAGE 5: APPLY TO ALPHA ==========
        logger.info("ğŸ“ Stage 5: Applying holes to alpha channel")
        
        alpha_float = alpha_array.astype(np.float32)
        
        # Complete transparency in hole centers
        alpha_float[holes_mask_smooth > 240] = 0
        
        # Graduated transparency in strong hole areas
        strong_holes = (holes_mask_smooth > 180) & (holes_mask_smooth <= 240)
        if np.any(strong_holes):
            alpha_factor = 1 - (holes_mask_smooth[strong_holes] / 255)
            alpha_float[strong_holes] *= alpha_factor
        
        # Smooth transition in edge zones
        if np.any(transition_zone):
            # Distance-based falloff
            distance_from_hole = cv2.distanceTransform(~(holes_mask > 200).astype(np.uint8), 
                                                       cv2.DIST_L2, 5)
            distance_norm = np.clip(distance_from_hole / 10.0, 0, 1)
            
            # Apply graduated transparency
            transition_alpha = distance_norm[transition_zone]
            alpha_float[transition_zone] *= transition_alpha
        
        # 5.1 Edge feathering for natural look
        # Create very smooth edges around holes
        for _ in range(2):
            alpha_temp = alpha_float.copy()
            alpha_temp[1:-1, 1:-1] = (
                alpha_float[1:-1, 1:-1] * 0.4 +
                alpha_float[:-2, 1:-1] * 0.15 +
                alpha_float[2:, 1:-1] * 0.15 +
                alpha_float[1:-1, :-2] * 0.15 +
                alpha_float[1:-1, 2:] * 0.15
            )
            # Only apply smoothing near holes
            smooth_mask = holes_dilated > 0
            alpha_float[smooth_mask] = alpha_temp[smooth_mask]
        
        alpha_array = np.clip(alpha_float, 0, 255).astype(np.uint8)
        
        logger.info("âœ… Ring holes made transparent with smooth transitions")
        
        # Log statistics
        holes_count = len(np.unique(labels)) - 1
        holes_area = np.sum(holes_mask > 0) / (h * w) * 100
        logger.info(f"ğŸ“Š Detected {holes_count} potential holes covering {holes_area:.1f}% of image")
    else:
        logger.info("â„¹ï¸ No ring holes detected")
    
    # Final composition
    a_new = Image.fromarray(alpha_array)
    result = Image.merge('RGBA', (r, g, b, a_new))
    
    # Verify RGBA mode
    if result.mode != 'RGBA':
        logger.error("âŒ WARNING: Result is not RGBA!")
        result = result.convert('RGBA')
    
    return result

def image_to_base64(image, keep_transparency=True):
    """Convert to base64 without padding - TRULY preserving transparency"""
    buffered = BytesIO()
    
    # CRITICAL FIX: Force RGBA and save as PNG
    if image.mode != 'RGBA' and keep_transparency:
        logger.warning(f"âš ï¸ Converting {image.mode} to RGBA for transparency")
        image = image.convert('RGBA')
    
    if image.mode == 'RGBA':
        logger.info("ğŸ’ Saving RGBA image as PNG with full transparency")
        # Save as PNG with NO compression for maximum transparency preservation
        image.save(buffered, format='PNG', compress_level=0, optimize=False)
    else:
        logger.info(f"Saving {image.mode} mode image as PNG")
        image.save(buffered, format='PNG', optimize=True, compress_level=1)
    
    buffered.seek(0)
    base64_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
    # Remove padding for Make.com compatibility
    return base64_str.rstrip('=')

def process_special_mode(job):
    """Process special modes - KOREAN ENCODING FIXED"""
    special_mode = job.get('special_mode', '')
    logger.info(f"ğŸ”¤ Processing special mode with FIXED Korean encoding: {special_mode}")
    
    # BOTH TEXT SECTIONS - Return TWO separate images
    if special_mode == 'both_text_sections':
        # Get text content with proper Korean encoding
        md_talk_text = job.get('md_talk_content', '') or job.get('md_talk', '') or """ê°ë„ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ë¹›ì˜ ê²°ë“¤ì´ ë‘ ì‚¬ëŒì˜ íŠ¹ë³„í•œ ìˆœê°„ìˆœê°„ì„ ë” ì°¬ë€í•˜ê²Œ ë§Œë“¤ë©° 360ë„ ìƒˆê²¨ì§„ íŒ¨í„´ìœ¼ë¡œ ë§¤ì¼ ìƒˆë¡œìš´ ë°˜ì§ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤ :)"""
        
        design_point_text = job.get('design_point_content', '') or job.get('design_point', '') or """ì…ì²´ì ì¸ ì»·íŒ… ìœ„ë¡œ ì„¬ì„¸í•˜ê²Œ ë¹›ë‚˜ëŠ” íŒ¨í„´ì´ ê³ ê¸‰ìŠ¤ëŸ¬ì›€ì„ ì™„ì„±í•˜ë©° ê°ì§„ í…ìŠ¤ì²˜ê°€ ì‹¬í”Œí•˜ë©´ì„œ ìœ ë‹ˆí¬í•œ ë§¤ë ¥ì„ ë”í•´ì¤ë‹ˆë‹¤."""
        
        # Handle encoding properly - FIXED
        if isinstance(md_talk_text, bytes):
            md_talk_text = md_talk_text.decode('utf-8', errors='replace')
        if isinstance(design_point_text, bytes):
            design_point_text = design_point_text.decode('utf-8', errors='replace')
        
        md_talk_text = str(md_talk_text).strip()
        design_point_text = str(design_point_text).strip()
        
        logger.info(f"âœ… Creating both Korean sections")
        logger.info(f"MD TALK text: {md_talk_text[:50]}...")
        logger.info(f"DESIGN POINT text: {design_point_text[:50]}...")
        
        # Create both sections with Korean support
        md_section = create_md_talk_section(md_talk_text)
        design_section = create_design_point_section(design_point_text)
        
        # Convert to base64
        md_base64_no_padding = image_to_base64(md_section, keep_transparency=False)
        design_base64_no_padding = image_to_base64(design_section, keep_transparency=False)
        
        # Return BOTH images separately
        return {
            "output": {
                "images": [
                    {
                        "enhanced_image": md_base64_no_padding,
                        "enhanced_image_with_prefix": f"data:image/png;base64,{md_base64_no_padding}",
                        "section_type": "md_talk",
                        "filename": "ac_wedding_004.png",
                        "file_number": "004",
                        "final_size": list(md_section.size),
                        "format": "PNG"
                    },
                    {
                        "enhanced_image": design_base64_no_padding,
                        "enhanced_image_with_prefix": f"data:image/png;base64,{design_base64_no_padding}",
                        "section_type": "design_point",
                        "filename": "ac_wedding_008.png",
                        "file_number": "008",
                        "final_size": list(design_section.size),
                        "format": "PNG"
                    }
                ],
                "total_images": 2,
                "special_mode": special_mode,
                "sections_included": ["MD_TALK", "DESIGN_POINT"],
                "version": VERSION,
                "status": "success",
                "korean_encoding": "UTF-8-FIXED",
                "korean_font_verified": KOREAN_FONT_VERIFIED,
                "korean_font_path": KOREAN_FONT_PATH
            }
        }
    
    # Single MD TALK section
    elif special_mode == 'md_talk':
        text_content = job.get('text_content', '') or job.get('claude_text', '') or job.get('md_talk', '')
        
        if not text_content:
            text_content = """ì´ ì œí’ˆì€ ì¼ìƒì—ì„œë„ ë¶€ë‹´ì—†ì´ ì°©ìš©í•  ìˆ˜ ìˆëŠ” í¸ì•ˆí•œ ë””ìì¸ìœ¼ë¡œ ë§¤ì¼ì˜ ìŠ¤íƒ€ì¼ë§ì— í¬ì¸íŠ¸ë¥¼ ë”í•´ì¤ë‹ˆë‹¤."""
        
        # Handle encoding - FIXED
        if isinstance(text_content, bytes):
            text_content = text_content.decode('utf-8', errors='replace')
        text_content = str(text_content).strip()
        
        logger.info(f"âœ… Creating MD TALK with Korean text: {text_content[:50]}...")
        
        section_image = create_md_talk_section(text_content)
        section_base64_no_padding = image_to_base64(section_image, keep_transparency=False)
        
        return {
            "output": {
                "enhanced_image": section_base64_no_padding,
                "enhanced_image_with_prefix": f"data:image/png;base64,{section_base64_no_padding}",
                "section_type": "md_talk",
                "filename": "ac_wedding_004.png",
                "file_number": "004",
                "final_size": list(section_image.size),
                "version": VERSION,
                "status": "success",
                "format": "PNG",
                "special_mode": special_mode,
                "korean_encoding": "UTF-8-FIXED",
                "korean_font_verified": KOREAN_FONT_VERIFIED,
                "korean_font_path": KOREAN_FONT_PATH
            }
        }
    
    # Single DESIGN POINT section
    elif special_mode == 'design_point':
        text_content = job.get('text_content', '') or job.get('claude_text', '') or job.get('design_point', '')
        
        if not text_content:
            text_content = """ë‚¨ì„± ë‹¨í’ˆì€ ë¬´ê´‘ í…ìŠ¤ì²˜ì™€ ìœ ê´‘ ë¼ì¸ì˜ ì¡°í™”ê°€ ê²¬ê³ í•œ ê°ì„±ì„ ì „í•˜ê³  ì—¬ì ë‹¨í’ˆì€ íŒŒë²  ì„¸íŒ…ê³¼ ì„¬ì„¸í•œ ë°€ê·¸ë ˆì¸ì˜ ë””í…Œì¼ë¡œ í™”ë ¤í•˜ë©´ì„œë„ ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ë°˜ì§ì„ì„ í‘œí˜„í•©ë‹ˆë‹¤."""
        
        # Handle encoding - FIXED
        if isinstance(text_content, bytes):
            text_content = text_content.decode('utf-8', errors='replace')
        text_content = str(text_content).strip()
        
        logger.info(f"âœ… Creating DESIGN POINT with Korean text: {text_content[:50]}...")
        
        section_image = create_design_point_section(text_content)
        section_base64_no_padding = image_to_base64(section_image, keep_transparency=False)
        
        return {
            "output": {
                "enhanced_image": section_base64_no_padding,
                "enhanced_image_with_prefix": f"data:image/png;base64,{section_base64_no_padding}",
                "section_type": "design_point",
                "filename": "ac_wedding_008.png",
                "file_number": "008",
                "final_size": list(section_image.size),
                "version": VERSION,
                "status": "success",
                "format": "PNG",
                "special_mode": special_mode,
                "korean_encoding": "UTF-8-FIXED",
                "korean_font_verified": KOREAN_FONT_VERIFIED,
                "korean_font_path": KOREAN_FONT_PATH
            }
        }
    
    else:
        return {
            "output": {
                "error": f"Unknown special mode: {special_mode}",
                "status": "error",
                "version": VERSION
            }
        }

def extract_file_number(filename: str) -> str:
    """Extract number from filename"""
    if not filename:
        return None
    
    match = re.search(r'(\d{3})', filename)
    if match:
        return match.group(1)
    
    match = re.search(r'(\d{2})', filename)
    if match:
        return match.group(1).zfill(3)
    
    return None

def find_input_data_fast(data):
    """Find input data - OPTIMIZED"""
    if isinstance(data, str) and len(data) > 50:
        return data
    
    if isinstance(data, dict):
        priority_keys = ['enhanced_image', 'image', 'image_base64', 'base64', 'img']
        
        for key in priority_keys:
            if key in data and isinstance(data[key], str) and len(data[key]) > 50:
                return data[key]
        
        for key in ['input', 'data', 'payload']:
            if key in data:
                if isinstance(data[key], str) and len(data[key]) > 50:
                    return data[key]
                elif isinstance(data[key], dict):
                    result = find_input_data_fast(data[key])
                    if result:
                        return result
        
        for i in range(10):
            key = str(i)
            if key in data and isinstance(data[key], str) and len(data[key]) > 50:
                return data[key]
    
    return None

def find_filename_fast(data):
    """Find filename - OPTIMIZED"""
    if isinstance(data, dict):
        for key in ['filename', 'file_name', 'name']:
            if key in data and isinstance(data[key], str):
                return data[key]
        
        for key in ['input', 'data']:
            if key in data and isinstance(data[key], dict):
                for subkey in ['filename', 'file_name', 'name']:
                    if subkey in data[key] and isinstance(data[key][subkey], str):
                        return data[key][subkey]
    
    return None

def decode_base64_fast(base64_str: str) -> bytes:
    """FAST base64 decode"""
    try:
        if not base64_str or len(base64_str) < 50:
            raise ValueError("Invalid base64 string")
        
        if 'base64,' in base64_str:
            base64_str = base64_str.split('base64,')[-1]
        
        base64_str = ''.join(base64_str.split())
        
        valid_chars = set(string.ascii_letters + string.digits + '+/=')
        base64_str = ''.join(c for c in base64_str if c in valid_chars)
        
        no_pad = base64_str.rstrip('=')
        
        try:
            decoded = base64.b64decode(no_pad, validate=False)
            return decoded
        except:
            padding_needed = (4 - len(no_pad) % 4) % 4
            padded = no_pad + ('=' * padding_needed)
            decoded = base64.b64decode(padded, validate=False)
            return decoded
            
    except Exception as e:
        logger.error(f"Base64 decode error: {str(e)}")
        raise ValueError(f"Invalid base64 data: {str(e)}")

def detect_pattern_type(filename: str) -> str:
    """Detect pattern type - Updated with AB pattern"""
    if not filename:
        return "other"
    
    filename_lower = filename.lower()
    
    if 'ac_' in filename_lower:
        return "ac_pattern"
    elif 'ab_' in filename_lower:
        return "ab_pattern"
    else:
        return "other"

def apply_pattern_enhancement_transparent(image: Image.Image, pattern_type: str) -> Image.Image:
    """Apply pattern enhancement while TRULY preserving transparency - AB 16%"""
    # CRITICAL: Ensure RGBA mode
    if image.mode != 'RGBA':
        logger.warning(f"âš ï¸ Converting {image.mode} to RGBA in pattern enhancement")
        image = image.convert('RGBA')
    
    # CRITICAL: Process RGB channels separately to preserve alpha
    r, g, b, a = image.split()
    rgb_image = Image.merge('RGB', (r, g, b))
    
    # Convert to array for processing
    img_array = np.array(rgb_image, dtype=np.float32)
    
    # Apply enhancements based on pattern type
    if pattern_type == "ac_pattern":
        logger.info("ğŸ” AC Pattern - Applying 12% white overlay")
        # Apply 12% white overlay
        white_overlay = 0.12
        img_array = img_array * (1 - white_overlay) + 255 * white_overlay
        img_array = np.clip(img_array, 0, 255)
        
        # Convert back to image
        rgb_image = Image.fromarray(img_array.astype(np.uint8))
        
        brightness = ImageEnhance.Brightness(rgb_image)
        rgb_image = brightness.enhance(1.005)
        
        color = ImageEnhance.Color(rgb_image)
        rgb_image = color.enhance(0.98)
        
        logger.info("âœ… AC Pattern enhancement applied")
    
    elif pattern_type == "ab_pattern":
        logger.info("ğŸ” AB Pattern - Applying 16% white overlay and cool tone")
        # Apply 16% white overlay
        white_overlay = 0.16  # Changed to 16%
        img_array = img_array * (1 - white_overlay) + 255 * white_overlay
        
        # Cool tone adjustment
        img_array[:,:,0] *= 0.96  # Reduce red
        img_array[:,:,1] *= 0.98  # Reduce green
        img_array[:,:,2] *= 1.02  # Increase blue
        
        # Cool color grading
        cool_overlay = np.array([240, 248, 255], dtype=np.float32)
        img_array = img_array * 0.95 + cool_overlay * 0.05
        
        img_array = np.clip(img_array, 0, 255)
        rgb_image = Image.fromarray(img_array.astype(np.uint8))
        
        color = ImageEnhance.Color(rgb_image)
        rgb_image = color.enhance(0.88)
        
        brightness = ImageEnhance.Brightness(rgb_image)
        rgb_image = brightness.enhance(1.005)
        
        logger.info("âœ… AB Pattern enhancement applied with 16% white overlay")
        
    else:
        logger.info("ğŸ” Other Pattern - Standard enhancement")
        brightness = ImageEnhance.Brightness(rgb_image)
        rgb_image = brightness.enhance(1.08)
        
        color = ImageEnhance.Color(rgb_image)
        rgb_image = color.enhance(0.99)
        
        sharpness = ImageEnhance.Sharpness(rgb_image)
        rgb_image = sharpness.enhance(1.4)
    
    # Apply common enhancements
    contrast = ImageEnhance.Contrast(rgb_image)
    rgb_image = contrast.enhance(1.05)
    
    # Apply sharpening - SINGLE APPLICATION with highest value
    sharpness = ImageEnhance.Sharpness(rgb_image)
    rgb_image = sharpness.enhance(1.6)
    
    # CRITICAL: Recombine with ORIGINAL alpha channel
    r2, g2, b2 = rgb_image.split()
    enhanced_image = Image.merge('RGBA', (r2, g2, b2, a))
    
    logger.info(f"âœ… Enhancement applied while preserving transparency. Mode: {enhanced_image.mode}")
    
    # Verify RGBA mode
    if enhanced_image.mode != 'RGBA':
        logger.error("âŒ WARNING: Enhanced image is not RGBA!")
        enhanced_image = enhanced_image.convert('RGBA')
    
    return enhanced_image

def resize_to_target_dimensions(image: Image.Image, target_width=1200, target_height=1560) -> Image.Image:
    """Resize image to target dimensions preserving transparency"""
    # CRITICAL: Ensure RGBA mode
    if image.mode != 'RGBA':
        logger.warning(f"âš ï¸ Converting {image.mode} to RGBA in resize")
        image = image.convert('RGBA')
    
    width, height = image.size
    
    img_ratio = width / height
    target_ratio = target_width / target_height
    
    expected_ratio = 2000 / 2600
    
    if abs(img_ratio - expected_ratio) < 0.05:
        resized = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
    else:
        if img_ratio > target_ratio:
            new_height = target_height
            new_width = int(target_height * img_ratio)
        else:
            new_width = target_width
            new_height = int(target_width / img_ratio)
        
        resized = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        if new_width != target_width or new_height != target_height:
            left = (new_width - target_width) // 2
            top = (new_height - target_height) // 2
            right = left + target_width
            bottom = top + target_height
            
            resized = resized.crop((left, top, right, bottom))
    
    # Verify RGBA mode
    if resized.mode != 'RGBA':
        logger.error("âŒ WARNING: Resized image is not RGBA!")
        resized = resized.convert('RGBA')
    
    return resized

def apply_swinir_enhancement_transparent(image: Image.Image) -> Image.Image:
    """Apply SwinIR enhancement while preserving transparency"""
    if not REPLICATE_CLIENT:
        logger.warning("SwinIR skipped - no Replicate client")
        return image
    
    try:
        logger.info("ğŸ¨ Applying SwinIR enhancement with transparency")
        
        # CRITICAL: Ensure RGBA mode
        if image.mode != 'RGBA':
            logger.warning(f"âš ï¸ Converting {image.mode} to RGBA for SwinIR")
            image = image.convert('RGBA')
        
        # Separate alpha channel
        r, g, b, a = image.split()
        rgb_image = Image.merge('RGB', (r, g, b))
        
        buffered = BytesIO()
        rgb_image.save(buffered, format="PNG", optimize=True, compress_level=1)
        buffered.seek(0)
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        img_data_url = f"data:image/png;base64,{img_base64}"
        
        output = REPLICATE_CLIENT.run(
            "jingyunliang/swinir:660d922d33153019e8c263a3bba265de882e7f4f70396546b6c9c8f9d47a021a",
            input={
                "image": img_data_url,
                "task_type": "Real-World Image Super-Resolution",
                "scale": 1,
                "noise_level": 10,
                "jpeg_quality": 50
            }
        )
        
        if output:
            if isinstance(output, str):
                response = requests.get(output)
                enhanced_image = Image.open(BytesIO(response.content))
            else:
                enhanced_image = Image.open(BytesIO(base64.b64decode(output)))
            
            # Recombine with original alpha channel
            r2, g2, b2 = enhanced_image.split()
            result = Image.merge('RGBA', (r2, g2, b2, a))
            
            logger.info("âœ… SwinIR enhancement successful with transparency preserved")
            
            # Verify RGBA mode
            if result.mode != 'RGBA':
                logger.error("âŒ WARNING: SwinIR result is not RGBA!")
                result = result.convert('RGBA')
            
            return result
        else:
            return image
            
    except Exception as e:
        logger.warning(f"SwinIR error: {str(e)}")
        return image

def process_enhancement(job):
    """Main enhancement processing - V32 Ultra Detailed"""
    logger.info(f"=== Enhancement {VERSION} Started ===")
    logger.info("ğŸ¯ ULTRA DETAILED: Enhanced 6-stage background removal")
    logger.info("ğŸ’ TRANSPARENT OUTPUT: Preserving alpha channel throughout")
    logger.info("ğŸ”¤ FIXED TEXT SECTIONS: 1200x600 with center alignment and margins")
    logger.info("ğŸ”§ AB PATTERN: Now using 16% white overlay")
    logger.info(f"Received job data: {json.dumps(job, indent=2)[:500]}...")
    start_time = time.time()
    
    try:
        # Check for special mode first
        if job.get('special_mode'):
            return process_special_mode(job)
        
        # Normal enhancement processing continues here...
        filename = find_filename_fast(job)
        file_number = extract_file_number(filename) if filename else None
        image_data = find_input_data_fast(job)
        
        if not image_data:
            return {
                "output": {
                    "error": "No image data found",
                    "status": "error",
                    "version": VERSION
                }
            }
        
        image_bytes = decode_base64_fast(image_data)
        image = Image.open(BytesIO(image_bytes))
        
        # Log initial image info
        logger.info(f"Input image mode: {image.mode}, size: {image.size}")
        
        # CRITICAL: Convert to RGBA immediately
        if image.mode != 'RGBA':
            logger.info(f"Converting {image.mode} to RGBA immediately")
            image = image.convert('RGBA')
        
        # STEP 1: ULTRA PRECISE BACKGROUND REMOVAL V2 - ALWAYS APPLY
        logger.info("ğŸ“¸ STEP 1: ALWAYS applying ULTRA PRECISE background removal V2")
        removal_start = time.time()
        image = u2net_ultra_precise_removal_v2(image)
        logger.info(f"â±ï¸ Ultra precise background removal V2 took: {time.time() - removal_start:.2f}s")
        
        # Verify RGBA after removal
        if image.mode != 'RGBA':
            logger.error("âŒ Image lost RGBA after background removal!")
            image = image.convert('RGBA')
        
        # STEP 2: ENHANCEMENT (preserving transparency)
        logger.info("ğŸ¨ STEP 2: Applying enhancements with TRUE transparency preservation")
        enhancement_start = time.time()
        
        # Auto white balance with alpha preservation
        def auto_white_balance_rgba(rgba_img):
            if rgba_img.mode != 'RGBA':
                rgba_img = rgba_img.convert('RGBA')
            
            r, g, b, a = rgba_img.split()
            rgb_img = Image.merge('RGB', (r, g, b))
            
            img_array = np.array(rgb_img, dtype=np.float32)
            
            gray_pixels = img_array[::15, ::15]
            gray_mask = (
                (np.abs(gray_pixels[:,:,0] - gray_pixels[:,:,1]) < 15) & 
                (np.abs(gray_pixels[:,:,1] - gray_pixels[:,:,2]) < 15) &
                (gray_pixels[:,:,0] > 180)
            )
            
            if np.sum(gray_mask) > 10:
                r_avg = np.mean(gray_pixels[gray_mask, 0])
                g_avg = np.mean(gray_pixels[gray_mask, 1])
                b_avg = np.mean(gray_pixels[gray_mask, 2])
                
                gray_avg = (r_avg + g_avg + b_avg) / 3
                
                img_array[:,:,0] *= (gray_avg / r_avg) if r_avg > 0 else 1
                img_array[:,:,1] *= (gray_avg / g_avg) if g_avg > 0 else 1
                img_array[:,:,2] *= (gray_avg / b_avg) if b_avg > 0 else 1
            
            rgb_balanced = Image.fromarray(np.clip(img_array, 0, 255).astype(np.uint8))
            r2, g2, b2 = rgb_balanced.split()
            return Image.merge('RGBA', (r2, g2, b2, a))
        
        image = auto_white_balance_rgba(image)
        
        # Detect pattern type
        pattern_type = detect_pattern_type(filename)
        detected_type = {
            "ac_pattern": "ë¬´ë„ê¸ˆí™”ì´íŠ¸(0.12)",
            "ab_pattern": "ë¬´ë„ê¸ˆí™”ì´íŠ¸-ì¿¨í†¤(0.16)",  # Changed to 16%
            "other": "ê¸°íƒ€ìƒ‰ìƒ(no_overlay)"
        }.get(pattern_type, "ê¸°íƒ€ìƒ‰ìƒ(no_overlay)")
        
        # Apply pattern-specific enhancements (preserving transparency)
        image = apply_pattern_enhancement_transparent(image, pattern_type)
        
        # ULTRA PRECISE ring hole detection V2
        logger.info("ğŸ” Applying ULTRA PRECISE ring hole detection V2")
        image = ensure_ring_holes_transparent_ultra_v2(image)
        
        logger.info(f"â±ï¸ Enhancement took: {time.time() - enhancement_start:.2f}s")
        
        # RESIZE (preserving transparency)
        image = resize_to_target_dimensions(image, 1200, 1560)
        
        # STEP 3: SWINIR ENHANCEMENT (preserving transparency)
        logger.info("ğŸš€ STEP 3: Applying SwinIR enhancement")
        swinir_start = time.time()
        image = apply_swinir_enhancement_transparent(image)
        logger.info(f"â±ï¸ SwinIR took: {time.time() - swinir_start:.2f}s")
        
        # Final verification
        if image.mode != 'RGBA':
            logger.error("âŒ CRITICAL: Final image is not RGBA! Converting...")
            image = image.convert('RGBA')
        
        logger.info(f"âœ… Final image mode: {image.mode}, size: {image.size}")
        
        # CRITICAL: NO BACKGROUND COMPOSITE - Keep transparency
        logger.info("ğŸ’ NO background composite - keeping pure transparency")
        
        # Save to base64 as PNG with transparency
        enhanced_base64_no_padding = image_to_base64(image, keep_transparency=True)
        
        # Build filename
        enhanced_filename = filename
        if filename and file_number:
            base_name = filename.rsplit('.', 1)[0] if '.' in filename else filename
            enhanced_filename = f"{base_name}_enhanced_transparent.png"
        
        total_time = time.time() - start_time
        logger.info(f"âœ… Enhancement completed in {total_time:.2f}s")
        
        output = {
            "output": {
                "enhanced_image": enhanced_base64_no_padding,
                "enhanced_image_with_prefix": f"data:image/png;base64,{enhanced_base64_no_padding}",
                "detected_type": detected_type,
                "pattern_type": pattern_type,
                "is_wedding_ring": True,
                "filename": filename,
                "enhanced_filename": enhanced_filename,
                "file_number": file_number,
                "final_size": list(image.size),
                "version": VERSION,
                "status": "success",
                "processing_time": f"{total_time:.2f}s",
                "has_transparency": True,
                "transparency_preserved": True,
                "background_removed": True,
                "background_applied": False,
                "format": "PNG",
                "output_mode": "RGBA",
                "korean_font_verified": KOREAN_FONT_VERIFIED,
                "korean_font_path": KOREAN_FONT_PATH,
                "special_modes_available": ["md_talk", "design_point", "both_text_sections"],
                "file_number_info": {
                    "001-003": "Enhancement",
                    "004": "MD TALK (1200x600)",
                    "005-006": "Enhancement",
                    "007": "Thumbnail",
                    "008": "DESIGN POINT (1200x600)",
                    "009-010": "Thumbnail",
                    "011": "COLOR section"
                },
                "enhancement_features": {
                    "background_removal": {
                        "version": "V32-ULTRA-DETAILED",
                        "stages": 7,
                        "features": [
                            "Pre-processing with contrast/sharpness enhancement",
                            "U2Net with alpha matting (threshold: 275)",
                            "Multi-scale edge detection (Fine/Medium/Coarse)",
                            "Triple guided filter passes",
                            "Reflection & metallic surface handling",
                            "Hair & fine detail recovery with Gabor filters",
                            "Artifact removal & hole filling",
                            "Final polish with edge feathering"
                        ]
                    },
                    "ring_hole_detection": {
                        "version": "V32-ENHANCED",
                        "stages": 5,
                        "criteria": 7,
                        "features": [
                            "Multi-color space analysis (HSV, LAB, RGB)",
                            "Hough Circle Transform detection",
                            "7-point confidence scoring system",
                            "Adaptive morphological cleaning",
                            "Smooth transition zones with distance transform",
                            "Edge feathering for natural holes"
                        ]
                    }
                },
                "optimization_features": [
                    "âœ… V32 ULTRA DETAILED: 6-stage background removal",
                    "âœ… ENHANCED ring hole detection with 7 validation criteria",
                    "âœ… Multi-scale edge detection for precision",
                    "âœ… Gabor filter hair detection",
                    "âœ… Reflection & metallic surface handling",
                    "âœ… AB PATTERN: Now using 16% white overlay",
                    "âœ… STABLE TRANSPARENT PNG: Verified at every step",
                    "âœ… WORKING FONT URLS: Google Fonts GitHub raw URLs",
                    "âœ… SIMPLIFIED ENCODING: No complex encoding conversions",
                    "âœ… OPTIMIZED: Single sharpening pass (1.6)",
                    "âœ… CRITICAL: RGBA mode enforced throughout",
                    "âœ… Ready for Figma transparent overlay",
                    "âœ… Pure PNG with full alpha channel",
                    "âœ… Make.com compatible base64 (no padding)",
                    "âœ… Text sections with professional layout"
                ],
                "processing_order": "1.U2Net-Ultra-V2 â†’ 2.Enhancement â†’ 3.SwinIR",
                "swinir_applied": True,
                "png_support": True,
                "edge_detection": "ULTRA PRECISE (Multi-scale + Guided Filter)",
                "korean_support": "COMPLETELY FIXED - No encoding parameter needed",
                "white_overlay": "AC: 12% | AB: 16% | Other: None",
                "expected_input": "2000x2600 (any format)",
                "output_size": "1200x1560",
                "transparency_info": "Full RGBA transparency preserved - NO background",
                "make_com_compatibility": "Base64 without padding"
            }
        }
        
        logger.info("âœ… Enhancement completed successfully with ULTRA DETAILED processing")
        return output
        
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        import traceback
        
        return {
            "output": {
                "error": str(e),
                "status": "error",
                "version": VERSION,
                "traceback": traceback.format_exc()
            }
        }

def handler(event):
    """RunPod handler function"""
    logger.info(f"Handler received event type: {type(event)}")
    logger.info(f"Handler received event: {json.dumps(event, indent=2)[:500]}...")
    
    # Handle different event structures
    if isinstance(event, dict):
        if 'input' in event:
            job = event['input']
        else:
            job = event
    else:
        job = event
    
    return process_enhancement(job)

# RunPod handler
runpod.serverless.start({"handler": handler})
